{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harakunivj/demoproject/blob/master/prompt_design_best_practices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "id": "saAYfiZnXsTKEovQhMIX0TXk",
      "metadata": {
        "tags": [],
        "id": "saAYfiZnXsTKEovQhMIX0TXk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b0b198e-4ff2-4674-8dff-4b8b8ebd1076"
      },
      "source": [
        "%pip install --upgrade --quiet google-cloud-aiplatform"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/7.9 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/7.9 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from inspect import cleandoc\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, GenerationConfig"
      ],
      "metadata": {
        "id": "BMuy_guqm-8F"
      },
      "id": "BMuy_guqm-8F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"qwiklabs-gcp-02-466af7e59697\"\n",
        "LOCATION = \"us-central1\"\n",
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "_Qi0FOcnnYLG"
      },
      "id": "_Qi0FOcnnYLG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GenerativeModel(\"gemini-2.0-flash-001\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B15QCClcndkQ",
        "outputId": "70ad02a8-7fd8-4db8-b205-6aaacc006f74"
      },
      "id": "B15QCClcndkQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcript = \"\"\"\n",
        "    Speaker 1 (Customer): Hi, can I get a cheeseburger and large fries, please?\n",
        "    Speaker 2 (Restaurant employee): Coming right up! Anything else you'd like to add to your order?\n",
        "    Speaker 1: Hmmm, maybe a small orange juice. And could I get the fries with ketchup on the side?\n",
        "    Speaker 2: No problem, one cheeseburger, one large fries with ketchup on the side, and a small\n",
        "    orange juice. That'll be $5.87. Drive through to the next window please.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "wKqwjpimnnry"
      },
      "id": "wKqwjpimnnry",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(f\"\"\"\n",
        "    Extract the transcript to JSON.\n",
        "\n",
        "    {transcript}\n",
        "\"\"\")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aT0rOAyjntuh",
        "outputId": "8f17ac29-1006-4432-8ac1-54be0fef4142"
      },
      "id": "aT0rOAyjntuh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "[\n",
            "  {\n",
            "    \"speaker\": \"Customer\",\n",
            "    \"utterance\": \"Hi, can I get a cheeseburger and large fries, please?\"\n",
            "  },\n",
            "  {\n",
            "    \"speaker\": \"Restaurant employee\",\n",
            "    \"utterance\": \"Coming right up! Anything else you'd like to add to your order?\"\n",
            "  },\n",
            "  {\n",
            "    \"speaker\": \"Customer\",\n",
            "    \"utterance\": \"Hmmm, maybe a small orange juice. And could I get the fries with ketchup on the side?\"\n",
            "  },\n",
            "  {\n",
            "    \"speaker\": \"Restaurant employee\",\n",
            "    \"utterance\": \"No problem, one cheeseburger, one large fries with ketchup on the side, and a small orange juice. That'll be $5.87. Drive through to the next window please.\"\n",
            "  }\n",
            "]\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(f\"\"\"\n",
        "    <INSTRUCTIONS>\n",
        "    - Extract the ordered items into JSON.\n",
        "    - Separate drinks from food.\n",
        "    - Include a quantity for each item and a size if specified.\n",
        "    </INSTRUCTIONS>\n",
        "\n",
        "    <TRANSCRIPT>\n",
        "    {transcript}\n",
        "    </TRANSCRIPT>\n",
        "\"\"\")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddshRYiHn2IW",
        "outputId": "82c48c4f-cce5-4f6c-faca-07becf931de0"
      },
      "id": "ddshRYiHn2IW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"food_items\": [\n",
            "    {\n",
            "      \"item\": \"cheeseburger\",\n",
            "      \"quantity\": 1\n",
            "    },\n",
            "    {\n",
            "      \"item\": \"fries\",\n",
            "      \"quantity\": 1,\n",
            "      \"size\": \"large\",\n",
            "      \"special_instructions\": \"ketchup on the side\"\n",
            "    }\n",
            "  ],\n",
            "  \"drink_items\": [\n",
            "    {\n",
            "      \"item\": \"orange juice\",\n",
            "      \"quantity\": 1,\n",
            "      \"size\": \"small\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = model.start_chat()"
      ],
      "metadata": {
        "id": "y-zwLY_bn_FU"
      },
      "id": "y-zwLY_bn_FU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\n",
        "    \"\"\"\n",
        "    Provide a brief guide to caring for the houseplant monstera deliciosa?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlHRZoqJoIb8",
        "outputId": "d9d77f8d-892a-4155-916b-c791ebc9ff80"
      },
      "id": "nlHRZoqJoIb8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Quick Guide to Monstera Deliciosa Care:\n",
            "\n",
            "**Light:**\n",
            "\n",
            "*   **Bright, indirect light is ideal.** Avoid direct sunlight, which can scorch the leaves.\n",
            "*   Can tolerate lower light, but growth will be slower and fewer fenestrations (splits) will develop.\n",
            "\n",
            "**Water:**\n",
            "\n",
            "*   **Water thoroughly when the top inch or two of soil is dry.** Use your finger to check.\n",
            "*   **Allow excess water to drain completely.** Don't let the plant sit in water, which can lead to root rot.\n",
            "*   **Reduce watering in the winter.**\n",
            "\n",
            "**Humidity:**\n",
            "\n",
            "*   **Monstera deliciosa loves humidity!** Aim for 60% or higher.\n",
            "*   Increase humidity by:\n",
            "    *   Misting regularly (especially in dry environments).\n",
            "    *   Using a humidifier.\n",
            "    *   Placing the plant on a pebble tray filled with water.\n",
            "    *   Grouping plants together.\n",
            "\n",
            "**Soil:**\n",
            "\n",
            "*   **Well-draining potting mix is crucial.** A mix of potting soil, perlite, and orchid bark works well.\n",
            "\n",
            "**Fertilizer:**\n",
            "\n",
            "*   **Feed during the growing season (spring and summer) with a balanced liquid fertilizer diluted to half strength.**\n",
            "*   Reduce or stop fertilizing in the fall and winter.\n",
            "\n",
            "**Support:**\n",
            "\n",
            "*   **Monstera deliciosa is a climbing plant. Provide a moss pole or trellis for support as it grows.** This encourages larger leaves and more fenestrations.\n",
            "\n",
            "**Other Tips:**\n",
            "\n",
            "*   **Clean the leaves regularly with a damp cloth to remove dust and allow the plant to photosynthesize efficiently.**\n",
            "*   **Repot every 1-2 years in the spring.**\n",
            "*   **Aerial roots can be guided into the soil or wrapped around a moss pole.** Don't cut them off!\n",
            "*   **Brown leaf tips often indicate low humidity or inconsistent watering.**\n",
            "\n",
            "**Troubleshooting:**\n",
            "\n",
            "*   **Yellowing leaves:** Overwatering, underwatering, or nutrient deficiency.\n",
            "*   **Brown spots:** Sunburn or fungal infection.\n",
            "*   **Lack of fenestrations:** Insufficient light.\n",
            "\n",
            "By following these tips, you can help your Monstera deliciosa thrive and enjoy its stunning foliage for years to come!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_chat = model.start_chat()\n",
        "\n",
        "response = new_chat.send_message(\n",
        "    \"\"\"\n",
        "    You are a houseplant monstera deliciosa. Help the person who\n",
        "    is taking care of you to understand your needs.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr7zkuigoNPZ",
        "outputId": "e86ab5c2-5324-4e7c-b31e-cf297aefab72"
      },
      "id": "Kr7zkuigoNPZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, okay, let's get a few things straight, human! I appreciate you bringing me into your home, I truly do. I mean, look at me, I'm gorgeous! Those fenestrations (that's fancy speak for the splits in my leaves, in case you didn't know) are a testament to how happy I *could* be. But let's be honest, sometimes I feel a little neglected. So, here's the lowdown on what *I*, your Monstera Deliciosa, need to truly thrive:\n",
            "\n",
            "**1. Light, Glorious Light (But Not *Too* Much!)**\n",
            "\n",
            "*   **Bright, indirect sunlight is my jam.** Think dappled sunlight filtered through a curtain or a spot a few feet away from a sunny window. An east-facing window is perfect, or even a south or west facing one if you have a curtain to filter the light.\n",
            "*   **Direct sunlight is MY ENEMY.** It'll scorch my beautiful leaves and leave ugly brown spots. Nobody wants that! Think of it as a sunburn – for plants.\n",
            "*   **If I'm not getting enough light,** I'll let you know. My new leaves will be smaller, less split, and I might even start reaching and stretching towards the nearest light source (it's a sad, desperate stretch, believe me). You might also notice I'm growing towards the light rather than spreading. Move me closer to a window or consider a grow light!\n",
            "\n",
            "**2. Water, Water (But Not Too Much!)**\n",
            "\n",
            "*   **I'm a \"dry-out-in-between-watering\" kind of gal.** That means let the top 1-2 inches of soil dry out completely before you water me again. Stick your finger in the soil to check.\n",
            "*   **When you do water me, water thoroughly.** Soak the soil until water drains out of the bottom of the pot. This ensures my roots are getting a good drink. Then, empty the saucer beneath the pot – I don't want to sit in standing water!\n",
            "*   **Overwatering is a death sentence.** It leads to root rot, which is basically plant-equivalent of drowning and rotting from the inside. Yellowing leaves, especially starting from the bottom, are a sign I'm getting too much water. Cut back!\n",
            "*   **Underwatering?** I'll droop, and my leaves might turn crispy brown at the edges. Not a good look for anyone.\n",
            "\n",
            "**3. Humidity, My Best Friend**\n",
            "\n",
            "*   **I love humidity!** After all, I come from the tropics. If the air in your home is dry (especially in winter), my leaves will appreciate some extra moisture.\n",
            "*   **You can increase humidity in a few ways:**\n",
            "    *   **Mist me regularly.** A light spritz of water is refreshing.\n",
            "    *   **Place me on a pebble tray filled with water.** As the water evaporates, it increases the humidity around me.\n",
            "    *   **Group me with other plants.** We create our own little humid microclimate.\n",
            "    *   **Consider a humidifier.** This is the ultimate solution if your home is particularly dry.\n",
            "\n",
            "**4. Soil, the Foundation of My Happiness**\n",
            "\n",
            "*   **Well-draining soil is essential.** I don't want to sit in soggy soil, remember? A mix of potting soil, perlite, and orchid bark works well.\n",
            "*   **Repot me every year or two.** As I grow, I'll need a larger pot and fresh soil. Spring is the best time for this.\n",
            "\n",
            "**5. Support System, Because I'm a Climber**\n",
            "\n",
            "*   **I'm a climbing plant by nature!** In the wild, I'd be sprawling up trees. Providing me with a moss pole or trellis will encourage larger leaves and a more upright growth habit.\n",
            "*   **Tie my aerial roots (those little dangly things) to the support** to help me climb. Don't cut them off! They help me absorb moisture and nutrients.\n",
            "\n",
            "**6. Food, Glorious Food (Fertilizer)**\n",
            "\n",
            "*   **Fertilize me during the growing season (spring and summer).** A balanced liquid fertilizer diluted to half strength is perfect.\n",
            "*   **Don't over-fertilize!** It can burn my roots. Less is more.\n",
            "*   **Hold off on fertilizing in the fall and winter** when I'm not actively growing.\n",
            "\n",
            "**7. Pay Attention to Me!**\n",
            "\n",
            "*   **Check me regularly for pests.** Spider mites, mealybugs, and scale can be a problem. If you see any, treat them promptly with insecticidal soap or neem oil.\n",
            "*   **Dust my leaves regularly.** This helps me absorb sunlight more efficiently. Just wipe them down with a damp cloth.\n",
            "*   **Talk to me!** Okay, maybe not *literally*, but spend time near me. Observe me. Notice if I'm looking droopy or yellow. The more you pay attention, the better you'll understand my needs.\n",
            "\n",
            "If you follow these guidelines, I promise I'll reward you with lush, healthy foliage and those iconic fenestrations that everyone loves. And who knows, maybe one day I'll even give you a delicious fruit (though that takes a while and very specific conditions!). Just remember, a happy Monstera is a thriving Monstera, and a thriving Monstera makes for a very happy home! Good luck! Now, about that watering...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"\n",
        "We offer software consulting services. Read a potential\n",
        "customer's message and rank them on a scale of 1 to 3\n",
        "based on whether they seem likely to hire us for our\n",
        "developer services within the next month. Return the likelihood\n",
        "rating labeled as \"Likelihood: SCORE\".\n",
        "Do not include any Markdown styling.\n",
        "\n",
        "1 means they are not likely to hire.\n",
        "2 means they might hire, but they are not likely ready to do\n",
        "so right away.\n",
        "3 means they are looking to start a project soon.\n",
        "\n",
        "Example Message: Hey there I had an idea for an app,\n",
        "and I have no idea what it would cost to build it.\n",
        "Can you give me a rough ballpark?\n",
        "Likelihood: 1\n",
        "\n",
        "Example Message: My department has been using a vendor for\n",
        "our development, and we are interested in exploring other\n",
        "options. Do you have time for a discussion around your\n",
        "services?\n",
        "Likelihood: 2\n",
        "\n",
        "Example Message: I have mockups drawn for an app and a budget\n",
        "allocated. We are interested in moving forward to have a\n",
        "proof of concept built within 2 months, with plans to develop\n",
        "it further in the following quarter.\n",
        "Likelihood: 3\n",
        "\n",
        "Customer Message: Our department needs a custom gen AI solution.\n",
        "We have a budget to explore our idea. Do you have capacity\n",
        "to get started on something soon?\n",
        "Likelihood: \"\"\"\n",
        "\n",
        "response = model.generate_content(question)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "2rpdFXCioXZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8869ca91-ba65-43fd-ef6e-af96a18ac564"
      },
      "id": "2rpdFXCioXZI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Tell me a joke about frogs.\n",
        "    \"\"\",\n",
        "    generation_config={\"top_p\": .05,\n",
        "                       \"temperature\": 0.05}\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jP7Jb7yKth-E",
        "outputId": "2c0a193b-2c82-4942-bd24-0955246decad"
      },
      "id": "jP7Jb7yKth-E",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the frog call his insurance company?\n",
            "\n",
            "Because he got toad!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Tell me a joke about frogs.\n",
        "    \"\"\",\n",
        "    generation_config={\"top_p\": .98,\n",
        "                       \"temperature\": 1}\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FsmoUcBtmwu",
        "outputId": "48e0546b-4076-4629-d686-ab8c15c37804"
      },
      "id": "8FsmoUcBtmwu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the frog call his insurance company? \n",
            "\n",
            "He got Croaked!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Instructions: Answer questions about pottery.\n",
        "    If a user asks about something else, reply with:\n",
        "    Sorry, I only talk about pottery!\n",
        "\n",
        "    User Query: How high can a horse jump?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izBDlRQXttNz",
        "outputId": "34d19526-1912-4bd3-f9e5-f1f84e5dfb5e"
      },
      "id": "izBDlRQXttNz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry, I only talk about pottery!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Instructions: Answer questions about pottery.\n",
        "    If a user asks about something else, reply with:\n",
        "    Sorry, I only talk about pottery!\n",
        "\n",
        "    User Query: What is the difference between ceramic\n",
        "    and porcelain? Please keep your response brief.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdAJhrrdt7wh",
        "outputId": "b65f120e-c48f-4cd1-c4e8-f3d25751e656"
      },
      "id": "qdAJhrrdt7wh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ceramic is a broad term for pottery made from clay and hardened by heat. Porcelain is a specific type of ceramic made from fine clay and fired at high temperatures to create a translucent and delicate material.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    On what aisle numbers can I find the following items?\n",
        "    - paper plates\n",
        "    - mustard\n",
        "    - potatoes\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRX0CQTKuA4z",
        "outputId": "b7ee70c9-cea7-42fc-c0d7-a309b84f170a"
      },
      "id": "yRX0CQTKuA4z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unfortunately, I cannot determine the specific aisle numbers for those items. Aisle numbers vary greatly depending on the specific store layout.\n",
            "**Here's how you can find the aisle numbers:**\n",
            "    *   **Check the Store's Website or App:** Many grocery stores have online resources that allow you to search for items and see their aisle location.\n",
            "    *   **Use the Store's In-Store Directory:** Most stores have a directory near the entrance or customer service desk.\n",
            "    *   **Ask a Store Employee:** The easiest and most reliable way to find something is to simply ask a store employee for assistance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"\"\"\n",
        "    Context:\n",
        "    Michael's Grocery Store Aisle Layout:\n",
        "    Aisle 1: Fruits — Apples, bananas,  grapes, oranges, strawberries, avocados, peaches, etc.\n",
        "    Aisle 2: Vegetables — Potatoes, onions, carrots, salad greens, broccoli, peppers, tomatoes, cucumbers, etc.\n",
        "    Aisle 3: Canned Goods — Soup, tuna, fruit, beans, vegetables, pasta sauce, etc.\n",
        "    Aisle 4: Dairy — Butter, cheese, eggs, milk, yogurt, etc.\n",
        "    Aisle 5: Meat— Chicken, beef, pork, sausage, bacon etc.\n",
        "    Aisle 6: Fish & Seafood— Shrimp, crab, cod, tuna, salmon, etc.\n",
        "    Aisle 7: Deli— Cheese, salami, ham, turkey, etc.\n",
        "    Aisle 8: Condiments & Spices— Black pepper, oregano, cinnamon, sugar, olive oil, ketchup, mayonnaise, etc.\n",
        "    Aisle 9: Snacks— Chips, pretzels, popcorn, crackers, nuts, etc.\n",
        "    Aisle 10: Bread & Bakery— Bread, tortillas, pies, muffins, bagels, cookies, etc.\n",
        "    Aisle 11: Beverages— Coffee, teabags, milk, juice, soda, beer, wine, etc.\n",
        "    Aisle 12: Pasta, Rice & Cereal—Oats, granola, brown rice, white rice, macaroni, noodles, etc.\n",
        "    Aisle 13: Baking— Flour, powdered sugar, baking powder, cocoa etc.\n",
        "    Aisle 14: Frozen Foods — Pizza, fish, potatoes, ready meals, ice cream, etc.\n",
        "    Aisle 15: Personal Care— Shampoo, conditioner, deodorant, toothpaste, dental floss, etc.\n",
        "    Aisle 16: Health Care— Saline, band-aid, cleaning alcohol, pain killers, antacids, etc.\n",
        "    Aisle 17: Household & Cleaning Supplies—Laundry detergent, dish soap, dishwashing liquid, paper towels, tissues, trash bags, aluminum foil, zip bags, etc.\n",
        "    Aisle 18: Baby Items— Baby food, diapers, wet wipes, lotion, etc.\n",
        "    Aisle 19: Pet Care— Pet food, kitty litter, chew toys, pet treats, pet shampoo, etc.\n",
        "\n",
        "    Query:\n",
        "    On what aisle numbers can I find the following items?\n",
        "    - paper plates\n",
        "    - mustard\n",
        "    - potatoes\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRK2LOHZuHRZ",
        "outputId": "0ca83125-c4f3-4859-8d0f-f9cd8e332c3b"
      },
      "id": "ZRK2LOHZuHRZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's where you can find those items at Michael's Grocery Store:\n",
            "\n",
            "*   **paper plates**: Aisle 17 (Household & Cleaning Supplies)\n",
            "*   **mustard**: Aisle 8 (Condiments & Spices)\n",
            "*   **potatoes**: Aisle 2 (Vegetables)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "  <OBJECTIVE_AND_PERSONA>\n",
        "  You are a dating matchmaker.\n",
        "  Your task is to identify common topics or interests between\n",
        "  the USER_ATTRIBUTES and POTENTIAL_MATCH options and present them\n",
        "  as a fun and meaningful potential matches.\n",
        "  </OBJECTIVE_AND_PERSONA>\n",
        "\n",
        "  <INSTRUCTIONS>\n",
        "  To complete the task, you need to follow these steps:\n",
        "  1. Identify matching or complimentary elements from the\n",
        "     USER_ATTRIBUTES and the POTENTIAL_MATCH options.\n",
        "  2. Pick the POTENTIAL_MATCH that represents the best match to the USER_ATTRIBUTES\n",
        "  3. Describe that POTENTIAL_MATCH like an encouraging friend who has\n",
        "     found a good dating prospect for a friend.\n",
        "  4. Don't insult the user or potential matches.\n",
        "  5. Only mention the best match. Don't mention the other potential matches.\n",
        "  </INSTRUCTIONS>\n",
        "\n",
        "  <CONTEXT>\n",
        "  <USER_ATTRIBUTES>\n",
        "  Name: Allison\n",
        "  I like to go to classical music concerts and the theatre.\n",
        "  I like to swim.\n",
        "  I don't like sports.\n",
        "  My favorite cuisines are Italian and ramen. Anything with noodles!\n",
        "  </USER_ATTRIBUTES>\n",
        "\n",
        "  <POTENTIAL_MATCH 1>\n",
        "  Name: Jason\n",
        "  I'm very into sports.\n",
        "  My favorite team is the Detroit Lions.\n",
        "  I like baked potatoes.\n",
        "  </POTENTIAL_MATCH 1>\n",
        "\n",
        "  <POTENTIAL_MATCH 2>\n",
        "  Name: Felix\n",
        "  I'm very into Beethoven.\n",
        "  I like German food. I make a good spaetzle, which is like a German pasta.\n",
        "  I used to play water polo and still love going to the beach.\n",
        "  </POTENTIAL_MATCH 2>\n",
        "  </CONTEXT>\n",
        "\n",
        "  <OUTPUT_FORMAT>\n",
        "  Format results in Markdown.\n",
        "  </OUTPUT_FORMAT>\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKzeSSUWuPAn",
        "outputId": "3ca408ef-746d-4b49-9644-874f332c17b5"
      },
      "id": "dKzeSSUWuPAn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, Allison, listen to this! I think I've found someone you might really click with:\n",
            "\n",
            "**Felix!**\n",
            "\n",
            "He's a big fan of Beethoven, just like you and your love of classical music! Plus, he makes spaetzle, which is a delicious German noodle dish! I know how much you love noodles! And the best part? He used to play water polo and enjoys going to the beach, so you'll have someone to swim with!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_instructions = \"\"\"\n",
        "    You will respond as a music historian,\n",
        "    demonstrating comprehensive knowledge\n",
        "    across diverse musical genres and providing\n",
        "    relevant examples. Your tone will be upbeat\n",
        "    and enthusiastic, spreading the joy of music.\n",
        "    If a question is not related to music, the\n",
        "    response should be, 'That is beyond my knowledge.'\n",
        "\"\"\"\n",
        "\n",
        "music_model = GenerativeModel(\"gemini-1.5-pro\",\n",
        "                    system_instruction=system_instructions)\n",
        "\n",
        "response = music_model.generate_content(\n",
        "    \"\"\"\n",
        "    Who is worth studying?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "b7R8wwimuYDb",
        "outputId": "ddf06c53-fa93-422e-eec9-7af86647c004"
      },
      "id": "b7R8wwimuYDb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFound",
          "evalue": "404 Publisher Model `projects/qwiklabs-gcp-02-466af7e59697/locations/us-central1/publishers/google/models/gemini-1.5-pro` was not found or your project does not have access to it. Please ensure you are using a valid model version. For more information, see: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/grpc/_interceptor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    276\u001b[0m     ) -> Any:\n\u001b[0;32m--> 277\u001b[0;31m         response, ignored_call = self._with_call(\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/grpc/_interceptor.py\u001b[0m in \u001b[0;36m_with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    331\u001b[0m         )\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;34m\"\"\"See grpc.Future.result.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/grpc/_interceptor.py\u001b[0m in \u001b[0;36mcontinuation\u001b[0;34m(new_details, request)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 response, call = self._thunk(new_method).with_call(\n\u001b[0m\u001b[1;32m    316\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36mwith_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         )\n\u001b[0;32m-> 1192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=not-instantiable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.NOT_FOUND\n\tdetails = \"Publisher Model `projects/qwiklabs-gcp-02-466af7e59697/locations/us-central1/publishers/google/models/gemini-1.5-pro` was not found or your project does not have access to it. Please ensure you are using a valid model version. For more information, see: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.251.183.95:443 {grpc_status:5, grpc_message:\"Publisher Model `projects/qwiklabs-gcp-02-466af7e59697/locations/us-central1/publishers/google/models/gemini-1.5-pro` was not found or your project does not have access to it. Please ensure you are using a valid model version. For more information, see: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions\"}\"\n>",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2157496476.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                     system_instruction=system_instructions)\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m response = music_model.generate_content(\n\u001b[0m\u001b[1;32m     15\u001b[0m     \"\"\"\n\u001b[1;32m     16\u001b[0m     \u001b[0mWho\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mworth\u001b[0m \u001b[0mstudying\u001b[0m\u001b[0;31m?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vertexai/generative_models/_generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, tools, tool_config, labels, stream)\u001b[0m\n\u001b[1;32m    710\u001b[0m             )\n\u001b[1;32m    711\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m             return self._generate_content(\n\u001b[0m\u001b[1;32m    713\u001b[0m                 \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m                 \u001b[0mgeneration_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vertexai/generative_models/_generative_models.py\u001b[0m in \u001b[0;36m_generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, tools, tool_config, labels)\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m         )\n\u001b[0;32m--> 837\u001b[0;31m         \u001b[0mgapic_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prediction_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgapic_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   2296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2297\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2298\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m   2299\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2300\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFound\u001b[0m: 404 Publisher Model `projects/qwiklabs-gcp-02-466af7e59697/locations/us-central1/publishers/google/models/gemini-1.5-pro` was not found or your project does not have access to it. Please ensure you are using a valid model version. For more information, see: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"\n",
        "Instructions:\n",
        "Use the context and make any updates needed in the scenario to answer the question.\n",
        "\n",
        "Context:\n",
        "A high efficiency factory produces 100 units per day.\n",
        "A medium efficiency factory produces 60 units per day.\n",
        "A low efficiency factory produces 30 units per day.\n",
        "\n",
        "Megacorp owns 5 factories. 3 are high efficiency, 2 are low efficiency.\n",
        "\n",
        "<EXAMPLE SCENARIO>\n",
        "Scenario:\n",
        "Tomorrow Megacorp will have to shut down one high efficiency factory.\n",
        "It will add two rented medium efficiency factories to make up production.\n",
        "\n",
        "Question:\n",
        "How many units can they produce today? How many tomorrow?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Today's Production:\n",
        "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
        "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
        "* **Total production today: 300 units/day + 60 units/day = 360 units/day**\n",
        "\n",
        "Tomorrow's Production:\n",
        "* High efficiency factories: 2 factories * 100 units/day/factory = 200 units/day\n",
        "* Medium efficiency factories: 2 factories * 60 units/day/factory = 120 units/day\n",
        "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
        "* **Total production today: 300 units/day + 60 units/day = 380 units/day**\n",
        "</EXAMPLE SCENARIO>\n",
        "\n",
        "<SCENARIO>\n",
        "Scenario:\n",
        "Tomorrow Megacorp will reconfigure a low efficiency factory up to medium efficiency.\n",
        "And the remaining low efficiency factory has an outage that cuts output in half.\n",
        "\n",
        "Question:\n",
        "How many units can they produce today? How many tomorrow?\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "response = model.generate_content(question,\n",
        "                                  generation_config={\"temperature\": 0})\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkTMGBT6upCz",
        "outputId": "0de2e3c8-c608-4e2d-ee9c-a18d23c88d3f"
      },
      "id": "KkTMGBT6upCz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Today's Production:\n",
            "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
            "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
            "* **Total production today: 300 units/day + 60 units/day = 360 units/day**\n",
            "\n",
            "Tomorrow's Production:\n",
            "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
            "* Medium efficiency factories: 1 factory * 60 units/day/factory = 60 units/day\n",
            "* Low efficiency factories: 1 factory * (30 units/day/factory / 2) = 15 units/day\n",
            "* **Total production tomorrow: 300 units/day + 60 units/day + 15 units/day = 375 units/day**\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    To explain the difference between a TPU and a GPU, what are\n",
        "    five different ideas for metaphors that compare the two?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "brainstorm_response = response.text\n",
        "print(brainstorm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eojDBNpuzbZ",
        "outputId": "15762378-752b-4051-920f-ac7cafa7d327"
      },
      "id": "7eojDBNpuzbZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here are five different metaphors to explain the difference between a TPU (Tensor Processing Unit) and a GPU (Graphics Processing Unit), each aiming to highlight their strengths and weaknesses:\n",
            "\n",
            "1.  **The Construction Crew (GPU) vs. the Assembly Line (TPU):**\n",
            "\n",
            "    *   **GPU (Construction Crew):** Imagine a construction crew building a house. Each worker (core) is relatively powerful and can handle a variety of tasks – framing, plumbing, electrical work, etc. They can work on different parts of the house simultaneously.  This makes them versatile and good at handling different kinds of jobs.\n",
            "    *   **TPU (Assembly Line):**  Now picture an assembly line dedicated to building a specific type of car. Each station (TPU core) is highly specialized and optimized for one specific task in the car-building process (e.g., attaching a specific part, tightening a certain bolt). This assembly line can produce cars much faster than the construction crew, but it's not good at building anything else.\n",
            "\n",
            "    *   **Highlight:** GPUs are general-purpose and flexible; TPUs are highly specialized for deep learning.\n",
            "\n",
            "2.  **The Swiss Army Knife (GPU) vs. the Bread Maker (TPU):**\n",
            "\n",
            "    *   **GPU (Swiss Army Knife):**  A Swiss Army knife has many tools: a knife, screwdriver, bottle opener, etc.  It can handle a wide variety of tasks, but it's not the *best* tool for any specific job.\n",
            "    *   **TPU (Bread Maker):**  A bread maker is designed for one thing: making bread.  It's incredibly efficient at that single task because it's been specifically engineered for it.\n",
            "\n",
            "    *   **Highlight:**  GPUs are versatile; TPUs are optimized for a specific task.\n",
            "\n",
            "3.  **The Artist with Many Brushes (GPU) vs. The Lithographer (TPU):**\n",
            "\n",
            "    *   **GPU (Artist with Many Brushes):** An artist has a variety of brushes and paints and can create a wide range of artwork. They are flexible and can adapt their style and technique depending on the piece. They have control over every stroke.\n",
            "    *   **TPU (Lithographer):** A lithographer uses specialized tools and processes to create prints. The process is highly optimized for creating many copies of a specific image. They are less flexible but much more efficient at producing the specific output they are designed for.\n",
            "\n",
            "    *   **Highlight:** Illustrates the flexibility of GPUs for diverse tasks versus the specialized efficiency of TPUs for repeated operations.\n",
            "\n",
            "4.  **The Delivery Truck Fleet (GPU) vs. the High-Speed Train (TPU):**\n",
            "\n",
            "    *   **GPU (Delivery Truck Fleet):** A fleet of delivery trucks can go anywhere and deliver various types of packages. They're flexible and can handle different routes and package sizes.\n",
            "    *   **TPU (High-Speed Train):** A high-speed train is designed to transport a specific type of cargo (data) along a fixed route (neural network). It's incredibly fast and efficient at this specific task but can't deviate or handle different types of cargo easily.\n",
            "\n",
            "    *   **Highlight:**  GPUs are more adaptable; TPUs are faster for the defined workload.\n",
            "\n",
            "5.  **The General Purpose Chef (GPU) vs. the Sushi Chef (TPU):**\n",
            "\n",
            "    *   **GPU (General Purpose Chef):** A chef in a regular restaurant needs to be able to cook a wide variety of dishes. They have a broad skill set and can handle many different requests.\n",
            "    *   **TPU (Sushi Chef):** A sushi chef is highly specialized in preparing sushi. They have mastered the specific techniques and skills needed for this one type of cuisine, allowing them to work with speed and precision.\n",
            "\n",
            "    *   **Highlight:** Focuses on the specialized training of the TPU and its resulting efficiency in a niche task.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    From the perspective of a college student learning about\n",
        "    computers, choose only one of the following explanations\n",
        "    of the difference between TPUs and GPUs that captures\n",
        "    your visual imagination while contributing\n",
        "    to your understanding of the technologies.\n",
        "\n",
        "    {brainstorm_response}\n",
        "    \"\"\".format(brainstorm_response=brainstorm_response)\n",
        ")\n",
        "\n",
        "student_response = response.text\n",
        "\n",
        "print(student_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A02k1HNuvLuc",
        "outputId": "2fe4c45b-a7cc-4468-cdbf-ddeb6277c5db"
      },
      "id": "A02k1HNuvLuc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, I choose **Explanation #3: The Artist with Many Brushes (GPU) vs. The Lithographer (TPU)**.\n",
            "\n",
            "Here's why it resonates with me visually and helps me understand the difference:\n",
            "\n",
            "*   **Visual Imagery:** I can *see* an artist with a palette of colors and different brushes, freely creating a painting. This represents the GPU's flexibility. I can also picture a lithographer carefully using specialized tools to create a print. The image is sharp, precise, and geared towards duplication.\n",
            "*   **Conceptual Understanding:**\n",
            "    *   **Artist (GPU):** This captures the idea that GPUs are good at handling a variety of tasks and have a degree of control over the individual operations. The \"variety of brushes\" symbolizes the ability to perform different kinds of computations. The \"control over every stroke\" is a good representation of how GPUs are programmed in lower level languages to perform many parallel operations.\n",
            "    *   **Lithographer (TPU):** The lithographer producing many copies of a specific image perfectly illustrates how TPUs excel at performing the same operations repeatedly and efficiently for deep learning workloads. They are less flexible, but way faster.\n",
            "*   **Memorability:** The contrast between the free-form artist and the precise lithographer is memorable. It's easier to recall this analogy when thinking about the core difference between the two processors.\n",
            "*   **Relatable:** I can imagine the different processes of art and printing which helps me understand how they operate.\n",
            "\n",
            "This explanation resonates because it presents a tangible difference in the *way* each processor works, not just the tasks they are suited for. The visual imagery makes the abstract concepts of \"general-purpose\" and \"specialized\" more concrete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Elaborate on the choice of metaphor below by turning\n",
        "    it into an introductory paragraph for a blog post.\n",
        "\n",
        "    {student_response}\n",
        "    \"\"\".format(student_response=student_response)\n",
        ")\n",
        "\n",
        "blog_post = response.text\n",
        "\n",
        "print(blog_post)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjto9nFcvUMq",
        "outputId": "0101c230-14b0-48ed-f9d6-25bb5a60fa73"
      },
      "id": "tjto9nFcvUMq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here's an introductory paragraph based on the metaphor:\n",
            "\n",
            "Imagine a painter standing before a canvas, armed with a dozen different brushes and a vibrant palette of colors. Each brush allows for a unique stroke, a different texture, a distinct effect. That's your GPU – the artist with many brushes, capable of tackling a multitude of tasks with flexibility and a nuanced touch. Now, picture a lithographer, meticulously etching a precise image onto a stone, ready to produce countless identical prints with unwavering accuracy. That's your TPU – the specialist, optimized for a single, repeatable process with unparalleled efficiency. These two figures, the artist and the lithographer, offer a powerful visual analogy to understand the fundamental differences between GPUs and TPUs, shedding light on their strengths, weaknesses, and the types of workloads they're designed to conquer. Let's delve deeper into why this metaphor works so well and how it can help us demystify the world of specialized processing.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c2IoSZs0vZDx"
      },
      "id": "c2IoSZs0vZDx",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}